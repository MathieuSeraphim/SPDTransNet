<h1 style="text-align: center;">Running the Model</h1>

**IMPORTANT:** much of the actions described here requires an understanding of the way that configuration files are
dealt with. Make sure you have read [the relevant documentation file](./1%20-%20Implemented%20Configuration%20Strategy.md),
as the relevant configuration files might not be linked directly to avoid repetition (as in most of the documentation
within this repository).

The interface between the `Model` and `DataModule` is assures by PyTorch's `Trainer`
[class](https://lightning.ai/docs/pytorch/1.9.5/common/trainer.html). Amongst others, we use the following
configuration:
- A single GPU per run;
- The `EarlyStopping` and `ModelCheckpoint` callbacks, configured to maximize the MF1 score;
- A validation check 8 times per training epoch, since those are lengthy due to data duplication, and convergence is
attained after around 4 epochs;
- a time limit of 19 hours and 30 minutes (linked to the time limits on runs using HPC resources), after which a final
validation check is performed before termination.

<h2 style="text-align: center;">Running with Chosen Hyperparameters</h2>

The model is meant to be ran in one of two configurations: either in standalone mode, or through learned
hyperparameters.  
Standalone mode refers to using separate configuration files for the `Model` and `DataModule` classes (located
[here](../../_1_configs/_1_5_models) and [here](../../_1_configs/_1_4_data_modules) respectively).  
For all runs, the generated PyTorch Lightning log folder contains a file called `hparams.yaml`. The expectation is for
this file to be renamed and relocated into
[the appropriate directory](../../_1_configs/_1_z_miscellaneous/execution/past_runs_hyperparameters). Using said file,
a run can be executed from these hyperparameters. This is mostly relevant when these hyperparameters result from a
hyperprameter research (see next section).

Running the program is done through the `command_line_runner.py` [script](../../command_line_runner.py), with the
following command line arguments:
- --execution_method: either "standalone" or "from_hparams", as explained above.
- --execution_type: "fit", "validate", "test" or "predict".
- --global_seed: optional, to set all RNG seeds within the architecture.
- --trainer_config_file: the name of the `.yaml` file within the appropriate folder.
- --trainer_config.logger_logs_location: absolute or relative path of the log folders to be generated by Lightning.
- --trainer_config.logger_logs_folder_name: the name of the directory containing the aforementioned folders.
- --trainer_config.logger_version: the identifier of the run. Ignore for the default behavior.
- --model_config_file: the name of the `.yaml` file within the appropriate folder. Only used during standalone
execution.
- --datamodule_config_file: the name of the `.yaml` file within the appropriate folder. Only used during standalone
execution.
- --hparams_config_file: the name of the `.yaml` file within the appropriate folder. Only used outside of standalone
execution (replaces the two configuration files above).
- --model_config.cross_validation_fold_index: unused holdover from an earlier iteration. Please ignore.
- --datamodule_config.batch_size: self-explanatory.
- --datamodule_config.cross_validation_fold_index: self-explanatory. Must be between 0 and 30 for MASS-SS3.

<h3 id="testing" style="text-align: center;">Analyzing Results</h3>

Each run may be analyzed in one of two ways: TensorBoard and our testing commands.

You can open the per-run statistics saved within the `lightning_logs` folder (or equivalent) through the following
command:  
`tensorboard --logdir [PATH TO]/lightning_logs`  
This will present you graphs containing statistics on the training and validation sets collected during training.

To generate a `.csv` file with these statistics, as well as running on the test set with the best parameters and
generating covariance matrices (all within the generated `_5_1_runs_analysis/output`
[folder](../../_5_execution/_5_1_runs_analysis/output)), you can run the `command_line_tester.py`
[script](../../command_line_tester.py), with the following command line argument:
- --run_analysis_config_name: the relevant testing configuration file within the `results_analysis`
[folder](../../_1_configs/_1_z_miscellaneous/execution/results_analysis).

Within said folder, the `hparams_to_track`
[subfolder](../../_1_configs/_1_z_miscellaneous/execution/results_analysis/hparams_to_track) contains configuration
files for selecting which hyperparameters to include in the `.csv` file, and in what order; whereas the `stats_to_track`
[subfolder](../../_1_configs/_1_z_miscellaneous/execution/results_analysis/stats_to_track) ontains configuration
files to specify the metrics from each run's best parameters set to include in said `.csv` file.

<h2 id="optuna" style="text-align: center;">Customizing Your Hyperparameter Research</h2>

In this repository, hyperparameter researches are made with the `Optuna` [tool](https://optuna.org/).  
As stated in [the main README file](../../README.md#hparam_research), the first step is to generate the database by
running the appropriate BASH script.

**IMPORTANT:** the current behavior of maximizing the validation MF1 is more-or-less hardcoded. To use another metric,
feel free to [contact us](mailto:mathieu.seraphim@unicaen.fr) and ask for assistance.

Running with Optuna is still done through the `command_line_runner.py` [script](../../command_line_runner.py) as above,
with the following additional command line arguments:
- --optuna_flag: a flag without argument, signaling that Optuna is used for the execution.
- --optuna.study_name: must match the study name defined in the aforementioned BASH file used to generate the database.
- --optuna.pruner.[n_startup_trials/n_warmup_steps/interval_steps]: pruner arguments, see the documentation for the
utilized `MedianPruner` [class](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.pruners.MedianPruner.html#optuna.pruners.MedianPruner)
for more details.
- --optuna.hparam_selection_config.[model/datamodule]: the utilized configurations for choosing the hyperparameters
to apply to the training.

These last configuration files are located withint the `execution/hyperparameter_selection` folder, more specifically
[here](../../_1_configs/_1_z_miscellaneous/execution/hyperparameter_selection/models) and 
[here](../../_1_configs/_1_z_miscellaneous/execution/hyperparameter_selection/data_modules) respectively.  

The way they are defined is fairly complex. The corresponding logic is encoded within the `optuna_suggest_hparams`
function of the `hparam_selection.py` [Python file](../../_5_execution/_5_2_optuna/hparam_selection.py).  
If you have trouble understanding it, again, feel free to [contact us](mailto:mathieu.seraphim@unicaen.fr).